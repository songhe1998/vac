import torch
import torch.nn as nn
import numpy as np
import re
import os

import torch.nn.functional as F
from torch.utils.data import DataLoader
from dataset_continuous import Dataset
from jiwer import wer

from iso_model import Iso

from steamship import Steamship
client = Steamship(workspace="gpt-4", api_key='3A10DADE-7EA6-4616-A683-E7B708B91AA9')
generator = client.use_plugin('gpt-4')

prompt_attri = '''Now I am going to give you a sentence where each word has a probability of 
appearing correctly in the sentence. Please edit the sign glosses and give me a sentence that 
makes sense and also follows the greek sign language grammar. You should remove the repeated 
glosses and do not change the order of the appearing glosses. Feel free to delete the words 
with lower probability if you think that's necessary. Remember, the most important thing is 
following the greek sign language grammar and let the edited sentence make sense. Give me 10 
choices each with a confidence score between 0-1.Notice that the words and probabilities are 
generated by an imperfect sign gloss recognition model and I am going to use your answer as 
labels to train the recognition model. Also, do not change the order of the words!! '''
prompt_attri = prompt_attri.replace('\n','')

prompt_sufi = " Just give me answers in the form of sentence:score, no explanation, no other things. The shorter the better"

def is_int(string):
    try:
        int(string)
        return True
    except:
        return False

def is_float(string):
    try:
        float(string)
        return True
    except:
        return False

def extract(message, gloss_dict, g2e_dict, en):
    if en:
        word_dict = {v:k for k,v in g2e_dict.items()}
    else:
        word_dict = gloss_dict
    answers = message.split('\n')
    clean = []

    try:
        for a in answers:

            if ':' not in a:
                floats = re.findall(r"[-+]?(?:\d*\.*\d+)", a)
                scores = []
                if len(floats) == 0:
                    continue
                else:
                    for f in floats:
                        if (not is_int(f)) and (float(f) < 1):
                            scores.append(f)

                    if len(scores) == 0:
                        continue
                    if len(scores) == 1:
                        score = float(scores[0])
                    if len(scores) > 1:
                        continue

                sent = a
                for f in floats:
                    sent = sent.replace(f,'')
                sent = re.sub(r'[^\w\s]', '', sent).strip()


            else:
                sent, score = a.split(':')
                sent = re.sub(r'[^\w\s]', '', sent)
                score = re.findall(r"[-+]?(?:\d*\.*\d+)", score)
                if len(score) == 0:
                    continue
                score = score[0]
                try:
                    score = float(score)
                except:
                    print(a)
                    continue

            c_sent = []
            for i, w in enumerate(sent.split()):
                if i==0 and is_float(w):
                    continue
                w = w.upper()
                if w in word_dict and w not in c_sent:
                    c_sent.append(w)
            c_sent = ' '.join(c_sent)

            if len(c_sent) > 0:
                item = (c_sent, score)
                clean.append(item)
    except:
      return []

    return clean


    
def read_csv(path):
    data = open(path).readlines()
    #names = data[0].replace('\n', '').split('|')
    names = ['path','gloss']
    save_arr = []
    for line in data:
        save_dict = {name: 0 for name in names}
        line = line.replace('\n', '').split('|')
        for name, item in zip(names, line):
            save_dict[name] = item
        save_arr.append(save_dict)
    return save_arr

def make_gloss_dict(paths):
    data = []
    for path in paths:
        res = read_csv(path)
        data.extend(res)

    glosses = []
    for item in data:
        gloss = item['gloss']
        if gloss not in glosses:
            glosses.append(gloss)
    gloss_dict = {g:i for i,g in enumerate(glosses)}
    i2g_dict = {i:g for i,g in enumerate(glosses)}
    return gloss_dict, i2g_dict

def make_continuous_gloss_dict(paths):
    data = []
    for path in paths:
        res = read_csv(path)
        data.extend(res)

    glosses = []
    for item in data:
        sent = item['gloss']
        for gloss in sent.split():
            if gloss not in glosses:
                glosses.append(gloss)
    gloss_dict = {g:i for i,g in enumerate(glosses)}
    i2g_dict = {i:g for i,g in enumerate(glosses)}
    return gloss_dict, i2g_dict

def make_dict(en_path, gk_path):
    with open(en_path) as ef:
        en_words = ef.readlines()
        en_words = [w.replace('\n','').upper() for w in en_words]
    with open(gk_path) as gf:
        gk_words = gf.readlines() 
        gk_words = [w.replace('\n','') for w in gk_words]
    g2e_dict = {}
    for e,g in zip(en_words, gk_words):
        g2e_dict[g] = e
    return g2e_dict

def translate(i, g2e_dict, en):
    if not isinstance(i, int):
        i = i.item()
    if en:
        w = i2g_dict[i]
        if w not in g2e_dict:
            w = w.replace('(1)','').replace('(2)','').replace('(3)','')
        if w not in g2e_dict:
            w = 'ΑΥΤΗ_ΔΙΝΩ_ΕΣΕΝΑ'
        word = g2e_dict[w]
    else:
        word = i2g_dict[i.item()]

    return word

#torch.manual_seed(1)

prefix = '../../GSL_continuous'
train_gt_path = '../../GSL_continuous_files/GSL_SI/gsl_split_SI_train.csv'
test_gt_path = '../../GSL_continuous_files/GSL_SI/gsl_split_SI_test.csv'
filter_key = 'ΤΕΤΑΡΤΟΝ ΜΑΡΤΥΡΑΣ ΤΑΥΤΟΤΗΤΑ ΔΙΚΟ_ΤΟΥ'
filter_key = None
filter_key = 'ΠΡΙΝ ΕΣΥ ΑΣΤΥΝΟΜΙΑ ΤΗΛΕΦΩΝΩ'
filter_key = None
filter_len = None

gloss_train_gt_path = '../../GSL_iso_files/si/train_greek_iso.csv'
gloss_test_gt_path = '../../GSL_iso_files/si/test_greek_iso.csv'
gloss_dict, i2g_dict = make_gloss_dict([gloss_train_gt_path, gloss_test_gt_path])

en_path = '../../bert_own/data/vocab_en.txt'
gk_path = '../../bert_own/data/vocab.txt'

g2e_dict = make_dict(en_path, gk_path)

print('number of glosses: ', len(gloss_dict))

test_dataset = Dataset(prefix, train_gt_path, gloss_dict, mode='test', filter_key=filter_key, filter_len=filter_len)


test_dataloader = DataLoader(
    test_dataset,
    batch_size=1,
    shuffle=False,
    drop_last=True,
    num_workers=0,  # if train_flag else 0
    collate_fn=test_dataset.collate_fn
    )


num_data_test = len(test_dataset)
print('number of test data: ', num_data_test)
#save_f = open('')


num_classes = len(gloss_dict)
hidden_size = 512
model = Iso(num_classes, hidden_size)
model.load_state_dict(torch.load('../work_dir_iso_sd/cnn_gsl_iso_11_0.878000020980835.pt'))
model.cuda()
model.eval()

en = True
save_snowball = False
second_round = False
if en:
    root = 'snowball_data_en'
else:
    root = 'snowball_data_gk'

if filter_key is not None:
    root = f'snowball_data_{filter_key}'

if not os.path.exists(root):
    os.makedirs(root)

print(f'Translating to English: {en}')
print(f'Saving snowball: {save_snowball}')
print(f'Using second round filter: {second_round}')
# Iterate over data.

raw_scores = []
avg_scores = []
high_conf_scores = []
best_scores = []
first_round_threshold_no_gpt_scores = []
second_round_threshold_no_gpt_scores = []

index_f = 0
first_round_threshold = 0.7 
second_round_threshold = 1.0

for data in test_dataloader:
    if index_f > 50:
        break

    padded_video, video_length, label, path = data
    path = path[0]
    mod_path = path.replace('/','_')
    if save_snowball:
        save_f = open(f'{root}/snowball_turn_one_{index_f}.txt','w')
    index_f += 1
    with torch.no_grad():
        output, lgt = model(x=padded_video.cuda(), len_x=video_length.cuda())
    pred = torch.argmax(output, dim=-1)
    raw = []
    new_sent = {}
    for index, i in enumerate(pred[0]):
        word = translate(i, g2e_dict, en)

        prob = torch.max((output/4.24).softmax(-1)[0][index]).item()

        if word not in new_sent:
            new_sent[word] = 0 
        new_sent[word] += prob

        # choices += f'{word} {prob}'
        if word not in raw:
            raw.append(word)

    choices = ''
    first_updated_label = []
    second_updated_label = []
    for word, prob in new_sent.items():
        if prob > first_round_threshold:
            prob = round(prob,4) 
            choices += f'{word} {prob}'

        if prob > first_round_threshold:
            first_updated_label.append(word)
        if prob > second_round_threshold:
            second_updated_label.append(word)
    first_round_threshold_no_gpt_sent = ' '.join(first_updated_label)          
    second_round_threshold_no_gpt_sent = ' '.join(second_updated_label)
    
    if save_snowball:
        save_f.write(f'path: {path}\n')

    label = ' '.join([translate(i, g2e_dict, en) for i in label[0]])
    first_round_threshold_no_gpt_score = wer(label, first_round_threshold_no_gpt_sent)
    second_round_threshold_no_gpt_score = wer(label, second_round_threshold_no_gpt_sent)

    raw_sent = ' '.join(raw)
    #print(choices)
    prompt = prompt_attri + choices + prefix

    task = generator.generate(text=prompt)
    task.wait()
    message = task.output.blocks[0].text
    answers = extract(message, gloss_dict, g2e_dict, en)

    if len(answers) == 0:
        print(f'SKIP\n{message}')
        continue

    answers = sorted(answers, key=lambda x:x[1], reverse=True)
    raw_score = wer(label, raw_sent)

    new_scores = []
    best_score = 10000
    for i, (sent, confi) in enumerate(answers):
        if second_round:
            sent_split = sent.split()
            words = []
            for w in sent_split:
                if w not in new_sent:
                    continue
                prob = new_sent[w]
                if prob > second_round_threshold:
                    words.append(w)
            sent = ' '.join(words)
        new_score = wer(label, sent)
        new_scores.append(new_score)
        line = f'{sent} {confi}\n'
        if save_snowball:
            save_f.write(line)
        if i == 0:
            high_conf_score = new_score
            high_conf_sent = sent
        if new_score < best_score:
            best_score = new_score
            best_sent = sent
    if save_snowball:
        save_f.close()

    avg_score = np.mean(new_scores)

    raw_scores.append(raw_score)
    avg_scores.append(avg_score)
    high_conf_scores.append(high_conf_score)
    best_scores.append(best_score)
    first_round_threshold_no_gpt_scores.append(first_round_threshold_no_gpt_score)
    second_round_threshold_no_gpt_scores.append(second_round_threshold_no_gpt_score)

    ma_raw = round(np.mean(raw_scores),4)
    ma_avg = round(np.mean(avg_scores),4)
    ma_high = round(np.mean(high_conf_scores),4)
    ma_best = round(np.mean(best_scores),4)
    ma_frt_ng = round(np.mean(first_round_threshold_no_gpt_scores),4)
    ma_srt_ng = round(np.mean(second_round_threshold_no_gpt_scores),4)

    print(f'Label: {label}')
    print(f'Raw: {raw_sent}')
    print(f'High: {high_conf_sent}')
    print(f'Best: {best_sent}')
    print(f'FRTNG: {first_round_threshold_no_gpt_sent}')
    print(f'SRTNG: {second_round_threshold_no_gpt_sent}')
    print(f'Moving average scores: ')
    print(f'Raw: {ma_raw}\nAvg: {ma_avg}\nHigh Confidence: {ma_high}\nBest: {ma_best}\nSecond Round Threshold No GPT: {ma_srt_ng}\nFirst Round Threshold No GPT: {ma_frt_ng} \n\n')
    if high_conf_score < second_round_threshold_no_gpt_score:
        print(high_conf_score, second_round_threshold_no_gpt_score)
        print('Gold sample')

print(len(avg_scores))


















